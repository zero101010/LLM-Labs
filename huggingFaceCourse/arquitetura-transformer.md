# Arquitetura de um Transformer

Junho de 2018: GPT, o primeiro modelo Transformer pré-treinado, usado para ajuste-fino em várias tarefas de NLP e obtendo resultados estado-da-arte.(https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)

Outubro de 2018: BERT, outro grande modelo pré-treinado, esse outro foi designado para produzir melhores resumos de sentenças(mais sobre isso no próximo capítulo!)
(https://arxiv.org/pdf/1810.04805)

Fevereiro de 2019: GPT-2, uma melhor (e maior) versão da GPT que não foi imediatamente publicizado o seu lançamento devido a preocupações éticas [N.T.: não apenas por isso]
(https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)

Outubro de 2019: DistilBERT, uma versão destilada do BERT que é 60% mais rápidam 40% mais leve em memória, e ainda retém 97% da performance do BERT

Outubro de 2019: BART e T5, dois grandes modelos pré-treinados usando a mesma arquitetura do modelo original Transformer (os primeiros a fazerem até então)

Maio de 2020, GPT-3, uma versão ainda maior da GPT-2 que é capaz de performar bem em uma variedade de tarefas sem a necessidade de ajuste-fino (chamado de aprendizagemzero-shot)(https://arxiv.org/pdf/2005.14165)

## Self attention
https://arxiv.org/pdf/1706.03762
